{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters\n",
    "\n",
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Gradient Boosting Regression is a machine learning algorithm that combines multiple weak predictive models (typically decision trees) in a sequential manner to create a strong predictive model. It is a type of boosting algorithm where each weak model is trained to correct the mistakes made by the previous models. Gradient Boosting Regression aims to minimize the loss function by iteratively adding models to the ensemble and adjusting their weights based on the gradients of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.86258582 7.86258582 7.86258582]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the loss function (mean squared error)\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Define the gradient of the loss function\n",
    "def mse_gradient(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / len(y_true)\n",
    "\n",
    "# Define a simple decision tree as a weak learner\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=1):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.feature_idx = 0\n",
    "        self.threshold = X.mean()\n",
    "        self.left_value = np.mean(y[X < self.threshold])\n",
    "        self.right_value = np.mean(y[X >= self.threshold])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(X < self.threshold, self.left_value, self.right_value)\n",
    "\n",
    "# Define the gradient boosting regressor\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the predictions with the mean of y\n",
    "        self.initial_prediction = np.mean(y)\n",
    "        predictions = np.full(len(y), self.initial_prediction)\n",
    "\n",
    "        # Build the ensemble of weak learners\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute the negative gradient (residuals)\n",
    "            residuals = mse_gradient(y, predictions)\n",
    "\n",
    "            # Train a decision tree on the negative gradient\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Update the predictions using the learning rate\n",
    "            predictions -= self.learning_rate * tree.predict(X)\n",
    "\n",
    "            # Add the tree to the ensemble\n",
    "            self.estimators.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Initialize the predictions with the mean of y\n",
    "        predictions = np.full(len(X), self.initial_prediction)\n",
    "\n",
    "        # Add the predictions of each tree in the ensemble\n",
    "        for tree in self.estimators:\n",
    "            predictions -= self.learning_rate * tree.predict(X)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "# Create a sample dataset\n",
    "X = np.array([1, 2, 3, 4, 5, 6])\n",
    "y = np.array([2, 4, 6, 8, 10, 12])\n",
    "\n",
    "# Create and fit the gradient boosting regressor\n",
    "regressor = GradientBoostingRegressor(n_estimators=10, learning_rate=0.1, max_depth=1)\n",
    "regressor.fit(X, y)\n",
    "\n",
    "# Predict on new data\n",
    "X_new = np.array([7, 8, 9])\n",
    "y_pred = regressor.predict(X_new)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the loss function (mean squared error)\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Define the gradient of the loss function\n",
    "def mse_gradient(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / len(y_true)\n",
    "\n",
    "# Define a simple decision tree as a weak learner\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=1):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.feature_idx = 0\n",
    "        self.threshold = X.mean()\n",
    "        self.left_value = np.mean(y[X < self.threshold])\n",
    "        self.right_value = np.mean(y[X >= self.threshold])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(X < self.threshold, self.left_value, self.right_value)\n",
    "\n",
    "# Define the gradient boosting regressor compatible with scikit-learn\n",
    "class GradientBoostingRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the predictions with the mean of y\n",
    "        self.initial_prediction = np.mean(y)\n",
    "        predictions = np.full(len(y), self.initial_prediction)\n",
    "\n",
    "        # Build the ensemble of weak learners\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute the negative gradient (residuals)\n",
    "            residuals = mse_gradient(y, predictions)\n",
    "\n",
    "            # Train a decision tree on the negative gradient\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Update the predictions using the learning rate\n",
    "            predictions -= self.learning_rate * tree.predict(X)\n",
    "\n",
    "            # Add the tree to the ensemble\n",
    "            self.estimators.append(tree)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Initialize the predictions with the mean of y\n",
    "        predictions = np.full(len(X), self.initial_prediction)\n",
    "\n",
    "        # Add the predictions of each tree in the ensemble\n",
    "        for tree in self.estimators:\n",
    "            predictions -= self.learning_rate * tree.predict(X)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "# Create a sample dataset\n",
    "X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)\n",
    "y = np.array([2, 4, 6, 8, 10, 12])\n",
    "\n",
    "# Create the gradient boosting regressor\n",
    "regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(regressor, param_grid, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding performance\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Score (MSE):\", -grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. In Gradient Boosting, a weak learner refers to a base model that performs slightly better than random guessing but is still relatively simple. In the context of Gradient Boosting Regression, weak learners are typically shallow decision trees with a small maximum depth. These trees individually have low predictive power but are combined in an ensemble to form a stronger predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. The intuition behind the Gradient Boosting algorithm is to iteratively add weak models to the ensemble, each correcting the mistakes made by the previous models. The algorithm focuses on reducing the residuals or errors of the previous models by fitting the new models to the negative gradients of the loss function. By combining multiple weak models in this way, Gradient Boosting is able to create a powerful ensemble model that can capture complex relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. Here's a high-level overview of the process:\n",
    "\n",
    "Initialize the predictions with the mean (or any other suitable value) of the target variable.\n",
    "Compute the negative gradient (residuals) of the loss function with respect to the current predictions.\n",
    "Train a weak learner (e.g., decision tree) to fit the negative gradient, aiming to minimize the residuals.\n",
    "Update the predictions by adding the learning rate multiplied by the predictions of the weak learner.\n",
    "Repeat steps 2-4 for a specified number of iterations (or until convergence).\n",
    "The final ensemble is formed by combining the predictions of all the weak learners.\n",
    "By iteratively adding weak models and updating the predictions, the ensemble gradually improves its ability to fit the data and reduce the overall loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. The steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm are as follows:\n",
    "\n",
    "Define a loss function: Choose an appropriate loss function that measures the discrepancy between the predicted values and the true values of the target variable. For regression problems, the mean squared error (MSE) is commonly used as the loss function.\n",
    "\n",
    "Initialize the ensemble: Set the initial predictions of the ensemble to a constant value, usually the mean of the target variable. This acts as the \"zeroth\" model in the ensemble.\n",
    "\n",
    "Compute the negative gradients: Calculate the negative gradients (also known as the residuals) of the loss function with respect to the current predictions. The negative gradient represents the direction and magnitude of the correction needed to reduce the loss.\n",
    "\n",
    "Train a weak learner: Fit a weak learner (e.g., decision tree) to the negative gradients. The weak learner is trained to approximate the relationship between the input features and the negative gradients, effectively correcting the mistakes made by the previous models. The weak learner is typically a shallow decision tree with limited depth.\n",
    "\n",
    "Update the predictions: Adjust the predictions of the ensemble by adding the learning rate multiplied by the predictions of the weak learner. The learning rate controls the contribution of each weak learner to the ensemble. A smaller learning rate makes the updates more conservative.\n",
    "\n",
    "Repeat steps 3-5: Iterate the process by computing new negative gradients based on the updated predictions and training additional weak learners to fit the negative gradients. Each new weak learner further improves the predictions of the ensemble by addressing the remaining errors or residuals.\n",
    "\n",
    "Final ensemble prediction: The final prediction is obtained by combining the predictions of all the weak learners in the ensemble. This is typically done by summing the predictions.\n",
    "\n",
    "By iteratively minimizing the loss function through the training of weak learners and updating the ensemble's predictions, the Gradient Boosting algorithm gradually builds a strong predictive model that can effectively capture complex patterns and relationships in the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
